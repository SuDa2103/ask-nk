{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d2f2c5-097f-4f02-b9dd-9365d6a23db6",
   "metadata": {},
   "source": [
    "# Build Embeddings\n",
    "\n",
    "## Dataset\n",
    "\n",
    "First we need to download the YT transcriptions dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58ca384-1f49-4565-ae1b-67c046dc38a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration jamescalam--youtube-transcriptions-dd7d2c78ffb97096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/jamescalam--youtube-transcriptions to /Users/jamesbriggs/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-dd7d2c78ffb97096/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ceddd01d93410d9936c6e0244eac63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac8e66b37884d77bdb20539222c0f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba450dc3c504553b86783f3e302e4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/jamesbriggs/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-dd7d2c78ffb97096/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'visibility', 'published', 'url', 'id', 'text', 'start', 'end'],\n",
       "    num_rows: 27214\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset  # !pip install datasets\n",
    "\n",
    "data = load_dataset(\n",
    "    \"jamescalam/youtube-transcriptions\",\n",
    "    split=\"train\",\n",
    "  \trevision=\"8dca835\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4f743c-4eb2-4e8d-b8a4-86f074f5d8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'visibility': 'Public',\n",
       " 'published': '2021-07-06 13:00:03 UTC',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'id': '35Pdoyi6ZoQ-t0.0',\n",
       " 'text': 'Hi, welcome to the video.',\n",
       " 'start': 0.0,\n",
       " 'end': 9.36}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f656fdc5-0d9b-404f-83ff-f03a1a15d1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'visibility': 'Public',\n",
       " 'published': '2021-07-06 13:00:03 UTC',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'id': '35Pdoyi6ZoQ-t3.0',\n",
       " 'text': 'So this is the fourth video in a Transformers',\n",
       " 'start': 3.0,\n",
       " 'end': 11.56}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3cae1e-3513-4c86-b9b6-9b274daabc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'visibility': 'Public',\n",
       " 'published': '2021-07-06 13:00:03 UTC',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'id': '35Pdoyi6ZoQ-t9.36',\n",
       " 'text': 'from Scratch mini series.',\n",
       " 'start': 9.36,\n",
       " 'end': 15.84}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec200e-3a1d-4dce-b3b7-08f72bb13c02",
   "metadata": {},
   "source": [
    "The sentences are all quite short at the moment, we need to merge them to create better chunks of text containing more meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f2876d-9e4a-4271-aeb7-4ac8afad7c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9072/9072 [00:07<00:00, 1156.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "new_data = []\n",
    "\n",
    "window = 6  # number of sentences to combine\n",
    "stride = 3  # number of sentences to 'stride' over, used to create overlap\n",
    "\n",
    "for i in tqdm(range(0, len(data), stride)):\n",
    "    i_end = min(len(data)-1, i+window)\n",
    "    if data[i]['title'] != data[i_end]['title']:\n",
    "        # in this case we skip this entry as we have start/end of two videos\n",
    "        continue\n",
    "    text = ' '.join(data[i:i_end]['text'])\n",
    "    new_data.append({\n",
    "        'start': data[i]['start'],\n",
    "        'end': data[i_end]['end'],\n",
    "        'title': data[i]['title'],\n",
    "        'text': text,\n",
    "        'id': data[i]['id'],\n",
    "        'url': data[i]['url'],\n",
    "        'published': data[i]['published']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c798ca-6aef-41a2-9979-056561ca693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.0,\n",
       " 'end': 25.76,\n",
       " 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data.\",\n",
       " 'id': '35Pdoyi6ZoQ-t0.0',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'published': '2021-07-06 13:00:03 UTC'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65d16ed-a84e-43a6-9798-9544248b6732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 981.4,\n",
       " 'end': 1009.52,\n",
       " 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'text': \"Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained\",\n",
       " 'id': '35Pdoyi6ZoQ-t981.4',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'published': '2021-07-06 13:00:03 UTC'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a49ea79-b7f7-4f74-9b79-09152fc21b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1096.0,\n",
       " 'end': 1112.0,\n",
       " 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)',\n",
       " 'text': \"token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here.\",\n",
       " 'id': 'x1lAcT3xl5M-t1096.0',\n",
       " 'url': 'https://youtu.be/x1lAcT3xl5M',\n",
       " 'published': '2021-05-27 16:15:39 UTC'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0496cd-ea41-4db4-b41a-98a495acaf05",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa18049-920c-4784-9240-fa251e58d3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "model = SentenceTransformer(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc0e39a1-266a-42ed-bede-e8dc3240476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1d1ae0-6f9d-4906-b84c-7bee0985b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 13392}},\n",
       " 'total_vector_count': 13392}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "index_id = \"youtube-search\"\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=\"<<YOUR_API_KEY>>\",  # app.pinecone.io\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "\n",
    "if index_id not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_id,\n",
    "        dim,\n",
    "        metric=\"dotproduct\"\n",
    "    )\n",
    "\n",
    "index = pinecone.Index(index_id)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828756a7-08a2-45be-a329-46100aced366",
   "metadata": {},
   "source": [
    "Now let's begin building the embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd0413e-ee5a-41b8-a5e1-1298cce62d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:56<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# we encode and insert in batches of 64\n",
    "batch_size = 64\n",
    "\n",
    "# loop through in batches of 64\n",
    "for i in tqdm(range(0, len(new_data), batch_size)):\n",
    "    # find end position of batch (for when we hit end of data)\n",
    "    i_end = min(len(new_data)-1, i+batch_size)\n",
    "    # extract the metadata like text, start/end positions, etc\n",
    "    batch_meta = [{\n",
    "        \"text\": new_data[x][\"text\"],\n",
    "        \"start\": new_data[x][\"start\"],\n",
    "        \"end\": new_data[x][\"end\"],\n",
    "        \"url\": new_data[x][\"url\"],\n",
    "        \"title\": new_data[x][\"title\"]\n",
    "    } for x in range(i, i_end)]\n",
    "    # extract only text to be encoded by embedding model\n",
    "    batch_text = [row['text'] for row in new_data[i:i_end]]\n",
    "    # create the embedding vectors\n",
    "    batch_embeds = model.encode(batch_text).tolist()\n",
    "    # extract IDs to be attached to each embedding and metadata\n",
    "    batch_ids = [row['id'] for row in new_data[i:i_end]]\n",
    "    # 'upsert' (eg insert) IDs, embeddings, and metadata to index\n",
    "    to_upsert = list(zip(batch_ids, batch_embeds, batch_meta))\n",
    "    index.upsert(to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a32f2d85-a310-4b97-bcf2-5993efecb784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 13392}},\n",
       " 'total_vector_count': 13392}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1c5458-c25a-4150-adc2-e783bcbe2b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'coaaSxys5so-t129.0',\n",
       "              'metadata': {'end': 157.0,\n",
       "                           'start': 129.0,\n",
       "                           'text': \"Let's ask another question. So this one's \"\n",
       "                                   \"not even really a question. I'm just going \"\n",
       "                                   'to say OpenAI Clip. And what I want to do '\n",
       "                                   'is just say okay can you summarize what '\n",
       "                                   'OpenAI Clip is.',\n",
       "                           'title': 'How to build next-level Q&A with OpenAI',\n",
       "                           'url': 'https://youtu.be/coaaSxys5so'},\n",
       "              'score': 33.6478119,\n",
       "              'values': []},\n",
       "             {'id': 'coaaSxys5so-t147.0',\n",
       "              'metadata': {'end': 183.0,\n",
       "                           'start': 147.0,\n",
       "                           'text': \"So we'll come down here. Let's see what it \"\n",
       "                                   'returns. Cool so OpenAI Clip is a '\n",
       "                                   'contrastive language image pre-training '\n",
       "                                   'model that uses pairs of images and text '\n",
       "                                   'and returns a matrix of cosine similarity '\n",
       "                                   \"between text and each image. Okay that's \"\n",
       "                                   'cool. So written in PyTorch uses bcelas.',\n",
       "                           'title': 'How to build next-level Q&A with OpenAI',\n",
       "                           'url': 'https://youtu.be/coaaSxys5so'},\n",
       "              'score': 31.5986061,\n",
       "              'values': []},\n",
       "             {'id': 'bVZJ_O_-t2085.44',\n",
       "              'metadata': {'end': 2131.7599999999998,\n",
       "                           'start': 2085.44,\n",
       "                           'text': \"OpenAI clip VIT so it's the vision \"\n",
       "                                   'transformer this VIT you see here refers '\n",
       "                                   'to the the vision transformer which clip '\n",
       "                                   'is using or is based on at least the '\n",
       "                                   'vision aspect and we want to write base '\n",
       "                                   \"patch 32. So I mean we'll go into more \"\n",
       "                                   'detail but the patch part of that is '\n",
       "                                   'referring to the way that the model almost '\n",
       "                                   'tokenizes your images it splits an image',\n",
       "                           'title': 'Intro to Dense Vectors for NLP and Vision',\n",
       "                           'url': 'https://youtu.be/bVZJ_O_-0RE'},\n",
       "              'score': 31.4537525,\n",
       "              'values': []},\n",
       "             {'id': '989aKUVBfbk-t35.0',\n",
       "              'metadata': {'end': 88.5,\n",
       "                           'start': 35.0,\n",
       "                           'text': 'During pre-training OpenAI trained the '\n",
       "                                   'model on pairs of images and text and it '\n",
       "                                   'trained them to both output embedding '\n",
       "                                   'vectors that are as close as possible to '\n",
       "                                   'each other. So the text transformer was '\n",
       "                                   'trained to output a single embedding 512 '\n",
       "                                   'dimensional embedding that was as close as '\n",
       "                                   \"possible to the vision transformer's image \"\n",
       "                                   'embedding for the image text pair. So what '\n",
       "                                   'that means is that clip is able to take '\n",
       "                                   'both images and text and embed them both '\n",
       "                                   'into a similar vector space. And with that '\n",
       "                                   'we can do a lot of things.',\n",
       "                           'title': 'Fast intro to multi-modal ML with '\n",
       "                                    \"OpenAI's CLIP\",\n",
       "                           'url': 'https://youtu.be/989aKUVBfbk'},\n",
       "              'score': 31.4496136,\n",
       "              'values': []},\n",
       "             {'id': '989aKUVBfbk-t98.0',\n",
       "              'metadata': {'end': 119.0,\n",
       "                           'start': 98.0,\n",
       "                           'text': 'OpenAI released a GitHub repository OpenAI '\n",
       "                                   \"clip here. This contains clip but we're \"\n",
       "                                   'not going to use this implementation. '\n",
       "                                   \"We're actually going to use this \"\n",
       "                                   'implementation of clip. So this is on '\n",
       "                                   'Hugging Face.',\n",
       "                           'title': 'Fast intro to multi-modal ML with '\n",
       "                                    \"OpenAI's CLIP\",\n",
       "                           'url': 'https://youtu.be/989aKUVBfbk'},\n",
       "              'score': 29.3169785,\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is OpenAI's CLIP?\"\n",
    "\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "index.query(xq, top_k=5, include_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6b0d0-1911-434f-ad72-33ec9ce567db",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
